---
title: "Demystifying Machine Learning - Training Exercises"
author: Alexander Lemm
output: 
  tutor::tutorial:
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
library(tutor)
library(httr)
library(ggplot2)
library(purrr)
library(dplyr)
library(tidyr)
library(randomForest)
library(rpart)
library(rpart.plot)
library(caret)
library(pmml)


tutor_options(exercise.completion = TRUE)
knitr::opts_chunk$set(echo = FALSE)
source("global.r")


```

## Overview

This course will give you a hands-on introduction to the typical steps of the data science process. Leveraging real-life credit data we will show you how to import, clean and explore data. Moreover, we will teach you how to model credit risk by using decision trees and random forests in R.  


## R Introduction

### FP vs OOP

Before we get started, you need to know a little bit about R and its underlying principles. R is a *functional programming language* in contrast to Java which is object-oriented and which most of you use on a daily basis. Usually, when you work with R, you use a functional programming style.

Functional languages make a lot of sense when you have a more or less fixed set of *things*, and as your code evolves, you primarily add new operations on existing things. For instance, R comes with 20 types of objects that represent the basic building blocks for data analysis. Surprisingly, those 20 objects are all you need for getting things done when analyzing data.

When you develop new stuff with a functional mindset you typically start by thinking about what your function should do. Next, you think about what objects get passed into the function as arguments and finally, you think about what objects are returned by the function.

When looking at code you can spot the difference between the two programming styles as follows: In an object-oriented language you would call a method of an object using the "dot" notation: *object.function(argument)*. In contrast, in a functional programming language the call would look as follows *function(object, argument)*.

In this course we will primarily deal with the following two object types: *data frames* and *atomic vectors*.

A *data frame* is used for storing data tables. Just think of it as being a single Excel sheet. Ideally, each line of the sheet represents one *observation/case*. Each column contains measurements on one variable which are specific for the respective observation. In R speech a data frame is a list of vectors of equal length. Continuing with the Excel analogy the sheet's columns are the vectors.

The *atomic vector* is the simplest R data type. Atomic vectors are linear vectors of a single primitive type. The four common types of atomic vectors are logical, integer, double, and character. Individual numbers or strings, which are scalars in other languages, are actually vectors of length one in R.

But enough theory for now, time to practice. In the following exercises you will learn some basic R commands to explore data frames and vectors in detail. 


### Exercises

#### Exercise 1

Every R session comes with a bunch of data sets which are directly available even though the user didn't load them explicitly into memory. We will take advantage of that fact and take a closer look at the `mtcars` data set. The data set was extracted from the 1974 Motor Trend US magazine, and comprises fuel consumption and aspects of automobile design and performance for several automobiles (1973â€“74 models).

In the console below type `mtcars` and hit CTRL + Enter or click on the 'Run Code' button. This will print the content of the data set to the console. 

```{r mtcars-print, exercise=TRUE}


```


#### Exercise 2

Check out the class of the mtcars object by typing `class(mtcars)`

```{r mtcars-class, exercise=TRUE}


```

```{r mtcars-class-quiz}

question("What is the class of the mtcars object?",
    answer("numeric vector"),
    answer("list"),
    answer("data frame", correct = TRUE),
    answer("matrix"),
    allow_retry = TRUE
  )


```


#### Exercise 3

Usually, you would not print the entire content of a data frame to the console like you did it in exercise 1. Instead, you would use the `head()` and `tail()` function to get a first glimpse on the data. By default, they return the first and respectively last 6 rows of a data frame. Try out both functions with the `mtcars` data frame below. 

```{r mtcars-head, exercise=TRUE}


```


#### Exercise 4

To get some information about the data frame's dimensions you can use `nrow()`, `ncol()`, and `dim()`. Before trying out all three functions in the console below, think a few seconds about them. What do you expect them to return?

```{r mtcars-dim, exercise=TRUE}


```

```{r mtcars-dim-quiz}

question("Of how many rows and columns does the mtcars data frame consist?",
    answer("32 columns and 11 rows"),
    answer("12 rows and 31 columns"),
    answer("32 rows and 11 columns", correct = TRUE)
  )


```


#### Exercise 5

One or perhaps THE most important function in R is `str()` which shows you the structure of an object. Try it out with the `mtcars` data frame.

*Note*: The `str()` function has nothing to do with strings! It always returns the **str**ucture of the object.

```{r mtcars-str, exercise=TRUE}


```

`str()` returns a brief description of the data: 

* The object's class
* The object's dimensionality
* a list of names of the columns including their type and their primary values 


#### Exercise 6

You can access an individual column of a specific data frame object, for instance named `my_data_frame`, using the `$` operator: `my_data_frame$desired_column`. 

In the console below execute `mtcars$mpg`. After that type `mtcars$` and hit 'tab'. This will show you all available columns of the data frame. Select 2-3 different columns and print their content to the console


```{r mtcars-dollar, exercise=TRUE}
mtcars$mpg

```
 

#### Exercise 7

If you like to assign a data frame column's content to a new variable you have to type the following: `new_vector <- my_data_frame$column_name`. Yes, your eyes didn't trick you. Instead of `=`, R uses `<-` as its assignment operator.

In the console below we give you a small example. Go ahead and execute the code. Afterwards, assign the the cylinder column to a new variable `my_cyl` and print it to the console. Compare this output with `mtcars$cyl`.

```{r mtcars-assignment, exercise=TRUE}
my_mpg <- mtcars$mpg

```



```{r mtcars-assignment-quiz}

question("What are the last 3 values of my_cyl when printing it to the console?",
    answer("6, 8, 4", correct = TRUE),
    answer("8, 8, 4"),
    answer("6, 6, 4")
  )

```


## Problem Introduction

### Some theory about credit risk modeling

Modeling credit risk for both personal and company loans/credits is of major importance for banks. Credit risk modeling is all about the event of loan default: the situation a client fails to repay all or parts of the loan/credit.

When a bank lends loan to a borrower, it usually transfers the entire amount of the loan to the borrower. Afterwards, the borrower will repay this loan amount in smaller chunks including some interest payments over time. There is a certain risk that a borrower will not fully repay his loan which results in a loss for the bank.

The components of expected loss (EL) for a bank consists of 3 parts:

* *Probability of default (PD)*: The probability a borrower will fail to fully repay his loan
* *Exposure at default (EAD)*:  Amount of loan which still needs to be repaid at time of default
* *Loss given default (LGD)*:  Amount of the loss at default expressed as percentage of the EAD

The expected loss is calculated by multiplying all three measurements: EL = PD x EAD x LGD.

In this course, however, we will only concentrate on the *probability of default (PD)*.

### Introducing the data

Banks keep information on the default behavior of past clients which can be used to predict defaults for new clients. This client information consists of two different types of information:

* Application information
    * Income
    * Marital status
    * ...
* Behavioral information (tracks past behavior of the client)
    * Payment arrears in account history 
    * Spending history
    * ...


The data of past clients which we will use in the remainder of the course to create Machine Learning models to predict the probability of default for new clients looks as follows:

*Note: If you see a blank window below, please reload the web page by clicking on 'Reload' in your browser.*

```{r}

head(readRDS("./www/loan_data_raw.rds"), 10)

```


The data set contains information on past loans. Each line represents one customer and his information along with the loan status indicator (`loan_status`) which equals 1 if the customer defaulted and 0 if the customer did not default. The other variables are as follows:

* `loan_amnt`: Amount of the loan
* `int_rate`: Interest rate
* `grade`: Bureau score reflecting credit risk history; A (highest credit worthiness) - G (lowest credit worthiness)
* `empl_length`: Employment length
* `home_ownership`: Home ownership status
* `annual_inc`: Annual income
* `age`: Age


### Exercises

#### Exercise 1

```{r crm-behavioral}

question("Which is the only behavioral variable in the dataset?",
         answer("int_rate"),
         answer("grade", correct = TRUE),
         answer("annual_inc"),
         answer("age")
         )

```




## Importing Data

### Different ways to import data

Right at the beginning you need to import your data into R. Otherwise you would not be able to do data science on it. Typically, that means that you import the data from a database, a file or from a web API into a data frame in R.

The simplest case is obviously importing data from a file. When data is stored in a .csv file you can use the `read.csv()` function to import it. The first argument to `read.csv()` is the name of the file including an absolute or relative path to it.

Right after importing the data you need to check if the import was successful. 


### Exercises


#### Exercise 1


We already loaded the complete raw loan data set for you into memory and stored in the variable `loan_data_raw`. So you don't need to execute `read.csv()` yourself. 

Use the console below to get your answers to the questions of the following quiz leveraging the functions you just learned about in section "R Introduction" (`str`, `head`, `tail`, `dim`, `class`). 

```{r check-out-loan-data, exercise=TRUE}

```


```{r check-out-loan-data-quiz}

quiz(caption = "Quiz: Examining loan data",
     question("How many observations are stored in loan_data_raw?",
              answer("8"),
              answer("19829"),
              answer("42"),
              answer("29092", correct = TRUE)
     ),
     question("How many variables are stored in loan_data_raw?",
              answer("19829"),
              answer("42"),
              answer("8", correct = TRUE),
              answer("29091")
     ),
     question("How many variables of type integer are stored in loan_data_raw?",
              answer("3"),
              answer("4", correct = TRUE),
              answer("1")
     ),       
     question("How many variables of type factor are stored in loan_data_raw?",
              answer("2", correct = TRUE),
              answer("4"),
              answer("1")
     ),
     question("What is the annual income of the last individual in the data set?",
              answer("22000", correct = TRUE),
              answer("200000"),
              answer("18000")
     ),
     question("What is the home ownership status of the second individual in the data set?",
              answer("OWN"),
              answer("RENT", correct = TRUE)
     )
)

```


#### Exercise 2 (Importing data easy is...)

OK, in the following 3 exercises we just gonna have some fun using the Star Wars REST API (http://swapi.co) to download some data directly from the web. This is just a little demo to show you that data scientists may also leverage any kind of web API to gather and analyze data.

```{r prepare-sw-api, warning=FALSE, message=FALSE}

parse_sw_people <- function(x) {
  x %>% httr::content(as = "text") %>% 
    jsonlite::fromJSON() %>% 
    .[["results"]] %>% 
    .[, 1:8] %>% 
    dplyr::mutate(
      height = as.numeric(height),
      mass = as.numeric(mass)
    )
}

people <- GET("http://swapi.co/api/people/") %>% parse_sw_people

sw_bmi <- people$mass / ((people$height/100)^2)

```

The code below will download and parse some Star Wars character related data from the web. Execute it and inspect the `people` variable afterwards simply by typing `people` in console. 


```{r sw-api-people, exercise=TRUE, exercise.setup = "prepare-sw-api", warning=FALSE, message=FALSE}

people <- GET("http://swapi.co/api/people/") %>% parse_sw_people

```

```{r sw-bmi-people-quiz}

question("Information on how many characters is stored in the people data frame?",
    answer("9"),
    answer("18"),
    answer("10", correct = TRUE)
  )


```



#### Excercise 3

Imagine you are a data scientist interested in the Body Mass Index (BMI) of Star Wars characters. With the data you just downloaded you now can calculate the BMI and directly add it as a new column to the existing `people` data frame. 

In general, the BMI is calculated as follows: bmi = (mass in kg) / ((height in m)^2). Fortunately, the math operators in R look exactly the same as in the formula above ("/", "^").

Execute the following code in the console below `sw_bmi <- people$mass / ((people$height / 100)^2)` and check out the `sw_bmi` variable afterwards. 


```{r sw-api-bmi, exercise=TRUE, exercise.setup = "prepare-sw-api"}




```

The command above calculated the BMI for each character based on the two columns `mass` and `height`. This command worked because in R most functions are "vectorized".

```{r sw-bmi-vector-quiz}

question("What is the second value in the sw_bmi vector?",
    answer("26.02758"),
    answer("26.89232", correct = TRUE),
    answer("25.08286")
  )


```




#### Exercise 4

We now can add the `sw_bmi` vector as a new column to the data frame `people` using  also the `$` operator and the `<-` assignment operator.

In the console below type `people$bmi <- sw_bmi` and inspect the entire data frame `people`.


```{r sw-api-new_column, exercise=TRUE, exercise.setup = "prepare-sw-api"}


```


```{r sw-bmi-quiz}

question("What is Darth Vader's BMI?",
    answer("34.00999"),
    answer("26.02758"),
    answer("33.33007", correct = TRUE)
  )


```

*Note:* What you just did is called 'feature engineering' in machine learning. Based on existing information you generated completely new information and added it to the data set.


## Exploring Data


Typically, the first step in data analysis is exploring the data using **simple descriptive statistics** and **graphical techniques**. Analysts also call this task exploratory data analysis (EDA for short). EDA is not a formal process with a strict set of rules but a very creative process which can vary a lot from data set to data set. 

As an analyst you try to achieve to following in the EDA:

* Develop a general understanding of, and "feeling" for,  your data
* Identify potential problems such as missing values and outliers
* Identify possible new features (feature engineering)
* Identify possible transformations

Actually, the easiest way to develop this "feeling" for your data is to use questions to lead your investigation. At the beginning of EDA you should feel free to investigate every idea and every question that comes to your mind. Some of these ideas will turn out all right, and probably a lot will be dead ends. The more questions you ask yourself the more quality questions you end up with. Each additional question that you ask will give you new insights.

There is no guideline or rule set about which questions you should ask to guide your research. However, the following types of questions are always a good starter for making discoveries within your data:

* What type of variation occurs within my variables?
* What is the relationship/behavior between my variables (covariation)?


### Exercises


#### Exercise 1

The workhorse function to retrieve many descriptive statistics in one go from a data frame (and also from other R objects) is `summary()`. It is usually the first function you apply when starting EDA. For numeric columns `summary()` returns five stats (Minimum, 1st quartile, median, 3rd quartile, maximum) and for categorical columns it returns either an excerpt or the complete frequency table. 

Please use `summary()` to start your EDA on `loan_data_raw` in the console below.

```{r eda-summary, exercise=TRUE}


```


```{r eda-summary-quiz}

quiz(caption = "Quiz: Using summary()",
     question("What is the minimum interest rate the bank used in the past?",
              answer("7.90"),
              answer("5.42", correct = TRUE),
              answer("10.99")
     ),
     question("What is strange when looking at the age variable?",
              answer("The youngest person who applied for a loan was 20 years old"),
              answer("The oldest person who applied for a loan was 144 years old", correct = TRUE),
              answer("The median age is 26 years")
     ),
     question("How many loan applicants owned property?",
              answer("2301", correct = TRUE),
              answer("12002"),
              answer("14692")
     )
)

```

*Important*: The summary output shows you the `NA's` category for both the `int_rate` and the `emp_length` variable. `NA` stands for "Not Available" and signals missing values. As the analyst you now know that for many observations these values are missing. This is an important finding you need to deal with later in the "Transforming Data" phase.


#### Excercise 2

`table()` is an extremely useful function when used with categorical data. It returns a frequency table of counts. For instance, the `home_owenership` and the `grade` columns are categorical variables in our data set. 

Apply `table()` on the `grade` column and remember the `$` operator which lets you select specific columns from a data frame.


```{r eda-table-one, exercise=TRUE}


```

```{r eda-table-one-hint}

table(loan_data_raw$grade)

```

```{r eda-table-one-quiz}

question("How many applicants had a credit score less or equal to F?",
              answer("211"),
              answer("267", correct = TRUE),
              answer("56")
     )

```


#### Exercise 3

When used with more than one categorical vector, `table()` returns a frequency table at each combination of the respective input levels. You just need to pass several categorical vectors to the function separated by a comma (`table(vector1, vector2)`). 

Below we would like to investigate the relationship between `grade` and `home_ownership`. Complete the command accordingly by adding the `home_ownership` variable to the `table()` function as the second argument. 

```{r eda-table-two, exercise=TRUE}

table(loan_data_raw$grade) 


```

```{r eda-table-two-hint}

table(loan_data_raw$grade, loan_data_raw$home_ownership) 


```


```{r eda-table-two-quiz}

question("How many applicants were home owners and had a credit score of 'D' when applying for their loan?",
              answer("434"),
              answer("1884"),
              answer("250", correct = TRUE)
     )

```


#### Exercise 4

There is one other important categorical variable in our data set: `loan_status`. If you look at the `summary()` output from exercise 1 again, it is not directly clear that `loan_status` is a categorical variable because all 5 stats for numeric variables are shown in the output. However, unlike R we know that `loan_status` only can have values "0" and "1" ("0" if the customer paid back the loan, "1" if he defaulted), so you can pass it to `table()`.

Besides the absolute counts returned by `table` an analyst might rather be interested in the proportions. You can achieve that by using `prop.table()` which requires a table as input: `prop.table(table(my_vector))`.

Please take a look at the proportions of `loan_status` in the console below.

```{r eda-table-three, exercise=TRUE}



```


```{r eda-table-three-quiz}

question("How many percent of all applicants defaulted in the data set?",
              answer("88.91 %"),
              answer("11.09 %", correct = TRUE)
)

```



#### Exercise 5

Now that you know the overall proportions of `loan_status` you might ask yourself the question if these proportions follow the same pattern across all bureau score classes (`grade` variable). Usually, you would expect that people with lower bureau scores default less often than people with higher scores.

Like with `table()` you can use `prop.table()` with more than one variable: `prop.table(table(vector1, vector2), margin = 1)`. In the console below please investigate the proportions of `loan_status` across the various levels of `grade` by executing the pre-filled code. 

```{r eda-table-four, exercise=TRUE}

prop.table(table(loan_data_raw$grade, loan_data_raw$loan_status), margin = 1)

```




```{r eda-table-four-quiz}

quiz(caption = "Quiz: Using prop.table() with two variables",
     question("How many percent of applicants in class 'C' paid back their loan?",
              answer("85.32 %", correct = TRUE),
              answer("79,72 %"),
              answer("82.05 %")
     ),
     question("What is the overall pattern you discovered?",
              answer("The proportions are the same across bureau score classes."),
              answer("People with a worse bureau score pay back their loan more often than the rest."),
              answer("People with a better bureau score pay back their loan more often than the rest.", correct = TRUE)
     )
)

```


#### Exercise 6

After exploring categorical variables we now focus on numeric variables in the subsequent exercises of this section. To plot the data we will use the `qplot()` function from the `ggplot2` package. 

A metarecipe for beginners to use this function looks as follows: `qplot(variable_on_x_axis, variable_on_y_axis_optional, data = your_data_frame, geom = "desired_plot_type")`. The `geom` parameter just specifies the plot type you would like to use. We will use one of the following valid `geom` values: "histogram", "density", "point":

* `geom = "histogram"` will create a histogram
* `geom = "density"` will create a density plot
* `geom = "point"` will create a scatterplot

One of the best ways to examine a single numeric variable to understand its variation is a histogram. A histogram is a graphical representation of the distribution of numerical data. For plotting a histogram you just need to specify the variable which will be plotted on the x-axis. The respective counts on the y-axis will be plotted automatically. 

The first variable we will examine a bit closer is `age`. Execute the code in the console below to create a histogram for the `age` variable. 


```{r eda-qplot-age, exercise=TRUE}

qplot(age, data = loan_data_raw, geom = "histogram")

```


```{r eda-qplot-age-quiz}

question("The range of values on the x-axis is fairly wide. What is the reason for it?",
              answer("Per default, qplot() creates the x-axis between 0 and 150"),
              answer("It is a clear sign of at least one outlier", correct = TRUE)
)

```




#### Exercise 7

One nice trick to take a closer look at the outliers of a single variable is a scatterplot. A scatterplot is a graph of plotted points that show the relationship between two variables. However, for outlier detection we can use a scatterplot with a single variable which will be plotted on the y-axis. On the x-axis we will just plot the number of the respective observation in the data set. 

To do so, please use `seq_along(age)` as x-axis parameter (don't forget about the y-axis parameter) and `geom = "point"` in the `qplot()` call. Check out the hint button should you struggle to create the plot.



```{r eda-qplot-age-point, exercise=TRUE}



```


```{r eda-qplot-age-point-hint}

qplot(seq_along(age), age , data = loan_data_raw, geom = "point")

```


```{r eda-qplot-age-point-quiz}

question("How many obvious age outliers could you spot?",
         answer("1", correct = TRUE),
         answer("8"),
         answer("42")
)

```



#### Exercise 8

The last variable we will examine is `loan_amnt`. Go ahead and create a histogram for `loan_amnt`.

```{r eda-qplot-loan_amnt, exercise=TRUE}



```


```{r eda-qplot-loan_amnt-hint}

qplot(loan_amnt , data = loan_data_raw, geom = "histogram")

```

#### Exercise 9

As an analyst you might be interested if the amount of credit people applied for differs between persons who defaulted and who did not. A very easy way to do this is to map a group variable to the `fill` or `colour` parameter in the `qplot()` call. 

Go ahead and re-create the histogram of the previous exercise. However, this time add `fill = as.factor(loan_status)` as the last argument to the `qplot()` call.

```{r eda-qplot-loan_amnt-two, exercise=TRUE}



```


```{r eda-qplot-loan_amnt-two-hint}

qplot(loan_amnt , data = loan_data_raw, geom = "histogram", fill = as.factor(loan_status))

```


#### Exercise 10

It is impossible to see if there is a real difference between the two groups when looking at the previous plot because the y-axis shows the absolute number of counts. Since there are many more people in the data set who paid their credit back you are not able to spot a difference in variation between the two groups.

In those situations analysts start using density plots instead of histograms. Density plots are much better for comparing distributions as they standardize the counts so that the areas under each frequency polygon is one.

Create a density plot for the `loan_amnt` variable using `loan_status` as the group variable. Just re-use the command of the previous exercise. Instead of using `geom = "histogram"` please specify `geom = "density"` and map `as.factor(loan_status)` to `colour` instead to `fill`: `colour = as.factor(loan_status)`.

```{r eda-qplot-loan_amnt-three, exercise=TRUE}



```


```{r eda-qplot-loan_amnt-three-hint}

qplot(loan_amnt , data = loan_data_raw, geom = "density", color = as.factor(loan_status))

```

```{r eda-qplot-loan_amnt-three-quiz}

question("Do you spot an obvious difference in variation of loan_amnt between poeple who defaulted and who did not?",
         answer("Yes, the distribution of loan amount for people who paid their credit back (red line) clearly differs from those who defaulted (blue line)."),
         answer("No, both density plots have very similar distributions. The amount of loan does not seem to be a strong indicator to separate those two groups.", correct = TRUE)
)

```

#### Exercise 11

OK, the last exercise showed that there does not seem to be an obvious difference in loan amount variation between the groups of people who defaulted and who did not. 

But what about people of different bureau score classes (`grade` variable)? Is the variation of loan amount the same across all of those classes or not?

To answer this question, we can map the `grade` variable to the `colour` parameter of the previous `qplot()` call (`colour = grade`). Like this you will create a single density plot with 7 lines. 

Alternatively, you can split your plot into facets. Facets are subplots that each display one subset of the data. In order to create your first facet plot in R, please re-use the `qplot()` command from the previous exercise. Just omit the `colour` parameter completely and add `facets  = ~ grade` as the last argument.

Go ahead and create both kind of density plots (one plot with 7 lines, 7 subplots with a single line each) by executing the code in the console below.

```{r eda-qplot-loan_amnt-four, exercise=TRUE}

qplot(loan_amnt , data = loan_data_raw, geom = "density", colour = grade)
qplot(loan_amnt , data = loan_data_raw, geom = "density", facets  = ~ grade)

```


```{r eda-qplot-loan_amnt-four-quiz}

question("Do you spot a difference in loan amount variation across people with different bureau scores?",
         answer("Yes, there is a difference. People with a higher bureau score who are less creditworthy tend to apply for loan amounts.", correct = TRUE),
         answer("No, there is no obvious difference in loan amount distribution across the groups."),
         answer("Yes, there is a difference. People with a lower bureau score who are more creditworthy tend to apply for higher credits.")
)

```

**Some word of caution:**  "Correlation does not imply causation" is a phrase used in statistics to emphasize that a correlation between two variables does not imply that one causes the other. In our case that means the following: It might be that these people default more because they ask for higher loan amounts, or they apply for higher loan amounts because they have bad credit scores and are desperate for a loan to consolidate credit card debts. More than that you are not able to tell at that point in time of the analysis.





## Transforming Data

The primary purpose when transforming the data is to address all issues identified during EDA. The analyst needs to make sure to prepare the data in such a way that it can directly be consumed by any ML algorithm further downstream.

It is very important to highlight that all steps conducted in this phase must be properly recorded because new data also needs to follow these pre-processing steps in any kind of production setup.

In particular, an analyst needs to address the following:

* Deal with missing values 
* Get rid of outliers if necessary
* Engineer new features and add them to the data
* Fully pre-process data so that it can go into modeling

In the lecture you discussed various ways how to deal with missing values in your data. In the following exercises we will cover another topic: outliers.

### Exercises

#### Exercise 1

In the previous section you identified one outlier in the `age` variable far beyond the 100 years threshold. This is clearly wrong data. In addition, we identified one outlier in the `annual_inc` variable in the lecture. That person had an annual income of $6.000.000. Even though an outlier in this data set $6.000.000 can be valid value in real life.

Let us first take a closer look at the one person in the data set representing the age outlier. For this we will use the `subset()` function. You can use the function for filtering/subsetting your data. For instance, to return a data frame with all persons who got a loan smaller than $20.000, you would use it in the following way: `subset(loan_data_raw, loan_amnt < 25)`.

Please use `subset` to return all persons who are older than 100 years in the data set.


```{r trans-subset, exercise=TRUE}



```


```{r trans-subset-quiz}

question("What is the interesting result?",
         answer("The person causing the outlier in the age variable got a C in bureau score."),
         answer("The observation causing the outlier in the age variable and the one responsible for the outlier in the annual_inc variable are one and the same.", correct = TRUE),
         answer("The person causing the outlier in the age variable got an interest rate of 12.73 %.")
)

```

#### Exercise 2

OK, you just learned that both outliers are actually caused by the same person. In this case we should delete the observation from the data set because at least the `age` entry is clearly wrong. 

Again, we can use the `subset()` function for this. This time, however, we will filter for all observations which are younger than 100 years. 

The code in the console below does the following:

* `dim(loan_data_raw)`: checks for the dimension of `loan_data_raw`
* `loan_data <- subset(loan_data_raw, age < 100)`: Subsets `loan_data_raw` by filtering for all persons younger than 100 years AND assigns the results to a new variable called `loan_data`
* `dim(loan_data)`: checks for the dimension of `loan_data`

Now, just execute the code and try to answer the question.

```{r trans-subset-two, exercise=TRUE}

dim(loan_data_raw)
loan_data <- subset(loan_data_raw, age < 100)
dim(loan_data)

```


```{r trans-subset-two-quiz}

question("What can you say about loan_data?",
         answer("The new data frame loan_data has the same dimension as loan_data_raw."),
         answer("The new data frame loan_data has one column less as expected."),
         answer("The new data frame loan_data has one observation less as expected.", correct = TRUE)
)

```


## Modeling Data

### Exercises

#### Exercise 1

```{r prepare-modeling}

# We will use 5 bins for emp_length and 5 bins for int_rate
loan_data <- loan_data_raw %>%
  mutate(
    emp_bin = as.character(cut_number(emp_length, 5)),
    int_bin = as.character(cut_number(int_rate, 5))
  ) %>%
  replace_na(list(emp_bin = "Missing", int_bin = "Missing")) %>%
  mutate(
    emp_bin = as.factor(emp_bin),
    int_bin = as.factor(int_bin),
    loan_status = as.factor(loan_status)
  ) %>%
  select(-emp_length, -int_rate)

set.seed(42)
index <- createDataPartition(loan_data$loan_status, p = 0.7, list = FALSE)
training_set <- loan_data[index, ]
test_set <- loan_data[-index, ]



rpart_mod <- partial(rpart, 
                     # Using priors
                     parms = list(prior = c(0.7, 0.3)),
                     control = rpart.control(cp = 0.001))

# Building forest using downsampling
randomForest_mod <- partial(randomForest, 
                             ## Tell randomForest to sample by strata. Here, 
                             ## that means within each class
                             strata = training_set$loan_status,
                             ## Now specify that the number of samples selected
                             ## within each class should be the same
                             sampsize = rep(sum(training_set$loan_status == 1), 2),
                             mtry = 3)



```

We already split the original data set `loan_data` into a training set and test set for you. Both are available within the consoles now as `training_set` and `test_set`. The training set consists of 70% of the original data and the test set of 30% respectively. In the first exercise you will train a classification tree using the training set. 

For building trees in R you use the `rpart()` function. Since `rpart()` comes with a lot of arguments which might be a bit overwhelming for beginners you will use the function `rpart_mod()`. It is just a modified version of `rpart()` we created for you with pre-filled arguments. 

Therefore, you just need to specify two arguments and pass them to `rpart_mod`. The first argument is a formula specifying the response (dependent variable) and the independent variables which should go into the model. The so-called formula interface to certain functions in R looks a bit weird because you have to use the "~" sign as you will see below.

Our response is `loan_status` and we would like to use all remaining variables as independent variables. Therefore **the first argument** looks as follows: `loan_status ~ .` "**.**" is just a shortcut for saying "all other variables". **The second argument** is the data the model should use for training: `data = training_set`.

Now it is your turn: Create your first classification tree and assign it to a new variable `my_tree`.

```{r model-tree, exercise=TRUE, exercise.setup="prepare-modeling"}



```

```{r model-tree-hint}

my_tree <- rpart_mod(loan_status ~ ., data = training_set)

```

```{r model-tree-quiz}

question("Congrats! You just created your first machine learning model in R. But what IS this classification tree actually?",
         answer("Your classification tree is just a snippet of an R script."),
         answer("I guess it is something magical."),
         answer("Your classifcation tree is an R object residing in memory.", correct = TRUE)
)

```

#### Exercise 2

Now it is time to take a closer look at the tree and by using the `prp()` function. Pass `my_tree` to `prp()` in the console.

```{r prepare-plot-tree}

# Prunning global my_tree
my_tree <- prune_tree(my_tree)

```



```{r plot-tree, exercise=TRUE, exercise.setup="prepare-plot-tree"}



```


```{r plot-tree-quiz}

question("Which variable was used for the first split?",
         answer("annual_income"),
         answer("grade", correct = TRUE),
         answer("age")
)

```

#### Exercise 3

Next, you will build your second machine learning model. However, this time you are going to build a whole random forest. But don't worry it is again just a single line of code.

Usually, you use the `randomForest` function to build the forest. Like in exercise 1 above we created a modified version of this function `randomForest_mod()` which comes with some pre-filled arguments you don't need to worry about here.

You just need to pass two arguments to the function exactly as before to `rpart_mod()`: `loan_status ~ .` and `data = training_set`. Go ahead and create your first random forest and assign it to the variable `my_forest`

*Note:* This may take a little bit of time because the forest will consist of 500 trees.

```{r model-forest, exercise=TRUE, exercise.setup="prepare-modeling"}



```

```{r model-forest-hint}

my_forest <- randomForest_mod(loan_status ~ ., data = training_set)

```



#### Exercise 4 

You cannot plot the forest in the same manner like a single tree. For instance, the forest you just built consists of 500 trees. Luckily, there is another plotting function available for random forests called `varImpPlot()`. Instead of plotting one or more trees this function produces a bar chart with the average variable importance measured across all trees. The scientific term here for the average variable importance is "MeanDecreaseGini".

Pass `my_forest` to `varImpPlot()` in the console.



```{r plot-forest, exercise=TRUE}



```

```{r plot-forest-quiz}

question("Which variable was used the most for splits in the entire forest?",
         answer("annual_income", correct = TRUE),
         answer("grade"),
         answer("age")
)

```

## Evaluating Models

Evaluating the model for credit risk means comparing the observed values in the loan status variable of the test set with the predicted outcomes.

A popular method for summarizing the results is called confusion matrix. It is a contingency table of correct and incorrect classifications. It tells us how "confused" the model is with the data. 

In the lecture you already saw the confusion matrices displaying the performance of the baseline model, the tree model and the random forest model applied on the test set. In order to do so we had to calculate the predictions produced by each model.

### Exercises

#### Exercise 1

In this exercise we will show you how to derive predictions in R and how to calculate a confusion matrix. 

Creating predictions in R is pretty straightforward: You need to use the `predict()` function for that. Its primary argument is a machine learning model and its second argument is new data which should be scored/predicted. The output is a vector of classified observations. 

The code below calculates predictions using the random forest model `my_forest` for the test data. The calculated predictions are stored in the `predict_forest` vector.  Afterwards the confusion matrix is calculated and stored in a new variable `table_forest`.

Read and try to understand the code below. Afterwards just execute it.


```{r predict-forest, exercise=TRUE, exercise.setup="prepare-modeling"}

predict_forest <- predict(my_forest, newdata = test_set)
table_forest <- table("Actual values" = test_set$loan_status, predictions = predict_forest)
table_forest

```

#### Exercise 2

Good, you just calculated predictions for the test set using the random forest model and compared them to the actual values using a confusion matrix. Next, you could calculate the different KPIs by hand you just learned in the lecture: accuracy, sensitivity and specificity. 

However, we created a little convenience function for you called `conf_kpis()` which just does this. Just pass the confusion matrix `table_forest` from the previous exercise to `conf_kpis` in the console below.  

```{r prepare-conf-matrix}

conf_kpis <- function(x) {
 accuracy <- sum(diag(x))/sum(x)
 specificity <- sum(x[1, 1]/sum(x[1, 1:2]))
 sensitivity <- sum(x[2, 2]/sum(x[2, 1:2]))
 list(accuracy = accuracy, specificity = specificity, sensitivity = sensitivity)
}


```





```{r calc-kpis, exercise=TRUE, exercise.setup="prepare-conf-matrix"}

```

```{r calc-kpis-hint}

conf_kpis(table_forest)

```

```{r calc-kpis-quiz}

question("What is the sensitivity of the random forest model applied on the test set?",
         answer("71 %"),
         answer("37 %", correct = TRUE),
         answer("76 %")
)

```


## Exporting Model

### Exercises

#### Exercise 1

In this exercise we just show you how to convert an R ML model (here: `my_tree`) into PMML which is super easy: You just need to put your ML model into the `pmml()` function and store the results in a new variable. 

Please execute the code below and take a look at the `tree_pmml` variable afterwards by printing it to the console. The additional two arguments `model.name` and `description` we pass to `pmml()` are optional but good practice.

**Note:** Do not try this for the random forest model here in the web application. It will time out and you won't receive a result because it actually takes several minutes to convert the entire forest to PMML.

```{r pmml-prepare}

my_tree <- prune_tree(my_tree)

```


```{r pmml, exercise = TRUE, exercise.setup="pmml-prepare"}

tree_pmml <- pmml(my_tree, 
                 model.name = "Credit Risk Model",
                 description = "Decision tree calculating the probablity of default.")


```

```{r pmml-quiz}

question("What is stored in the tree_pmml variable?",
         answer("A JSON presentation of your classification tree"),
         answer("A XML represenation of your classification tree", correct = TRUE)
)

```


#### Exercise 2

Next, we will save the results to disk using the `saveXML()` function. You just need to pass two arguments to `saveXML()`: the R object representing a XML document and the name of the file (including the absolute or relative path to it) to which we would like to save the XML document.  

Now, please execute the code below in the console. It will save the PMML representation of the tree model `tree_pmml` you created in the previous exercise to your disk.


```{r pmml-export-prepare}

my_tree <- prune_tree(my_tree)
tree_pmml <- pmml(my_tree, 
                  model.name = "Credit Risk Model",
                  description = "Decision tree calculating the probablity of default.")

```


```{r pmml-export, exercise=TRUE, exercise.setup="pmml-export-prepare"}

saveXML(tree_pmml, file = file.path(my_working_directory, 
                                    "export", "credit_risk_classification_tree.xml"))

```

**Note**: Normally, it would have been OK to specify the second argument to `saveXML()` like this: `file = "./export/credit_risk_classification_tree.xml`. However, each of the little consoles you used throughout the course runs in its own mini R process with a separate working directory somewhere in the temp folder hierarchy of your operating system. Therefore, we stored the working directory of the RStudio project in the `my_working_directory` variable during the startup of the web application. Like this, we were able to reference it later in order to store the xml file to the `/export` folder.


#### Exercise 3

In RStudio go to the `Files` tab and navigate to `./data_science_intro_tutorial/export`. Verify that the PMML file `credit_risk_classification_tree.xml` was created and check out its time stamp. Click on it so that it opens in the source pane


```{r pmml-export-quiz}

question("How many lines of code does the file have?",
         answer("Approximately 60"),
         answer("Approximately 250", correct = TRUE),
         answer("Approximately, 800")
)

```

